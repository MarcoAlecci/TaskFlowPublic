{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üè∑Ô∏è Phase 2 : Task-Driven Customization Phase\n",
    "\n",
    "Label-Driven Filtering + Task-Driven Refinement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from   dotenv   import load_dotenv\n",
    "import pandas   as pd\n",
    "import datetime\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Custom Imports\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "import AnalysisUtils\n",
    "import LLMUtils "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TMP_PATH = \"../0_Data/TMP\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"‚ö° START: {} ‚ö°\".format(datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")))\n",
    "initTime = datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TMP Folder\n",
    "if not os.path.exists(TMP_PATH):\n",
    "\tos.makedirs(TMP_PATH)\n",
    "\tprint(\"--- üìÅüÜï Folder created       : {}\\n\".format(TMP_PATH))\n",
    "else:\n",
    "\tprint(\"--- üìÅ‚úÖ Folder already exists: {}\\n\".format(TMP_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load .env Info\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üì• 1) Load Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model to be used\n",
    "MODEL = \"gpt-4o-mini\"\n",
    "\n",
    "# Number of clusters\n",
    "N_CLUSTERS = 150\n",
    "\n",
    "# TEST\n",
    "N_CLUSTERS = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Path\n",
    "DATA_PATH = \"./0_PipelineData/4_methodsPrivacyLabels_{}.csv\".format( MODEL)\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "methodsDF = pd.read_csv(DATA_PATH)\n",
    "\n",
    "# Print Number of Methods\n",
    "print(\"--- #Ô∏è‚É£ Methods Size: {}\".format(methodsDF.shape[0]))\n",
    "\n",
    "# Test purposes\n",
    "# methodsDF = methodsDF.head(3)\n",
    "\n",
    "methodsDF.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count distinct values for the column matching the current clusterID\n",
    "clusterID = \"clusterID_{}\".format(N_CLUSTERS)\n",
    "\n",
    "labelCounts = methodsDF[clusterID].value_counts()\n",
    "print(\"--- üîç Privacy Label Counts for column {}:\".format(clusterID))\n",
    "print(\"--- #Ô∏è‚É£ Number of unique labels : {}\".format(labelCounts.nunique()))\n",
    "for label, count in labelCounts.items():\n",
    "\tprint(\"------ {:<35}: {}\".format(label, count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  üîí 2] Read Privacy-Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predefined Privacy Labels\n",
    "PRIVACY_LABELS_FILE = \"./0_PipelineData/4_privacyLabels_{}.json\".format(MODEL)\n",
    "\n",
    "# Read all the privacy labels from the JSON file\n",
    "with open(PRIVACY_LABELS_FILE, 'r') as file:\n",
    "\tprivacyLabelsDict = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üñ•Ô∏è 3A) Using LLM to get a sublist of privacy-labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- ‚≠ï LLM Init & Check\")\n",
    "print(\"--- ‚≠ï Model: {}\".format(MODEL))\n",
    "\n",
    "# OpenAI (PAYING)\n",
    "llmInterface = LLMUtils.ChatGPTInterface(model=MODEL, pricing=0.150)\n",
    "print(llmInterface.sendRequest(\"Ping!\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "DOCUMENTATION_PATH = \"../0_Data/methodsDocumentationFiles/\"\n",
    "\n",
    "# Number of replies to get\n",
    "NUM_REPLIES = 5\n",
    "\n",
    "# Max requests to send to LLMs\n",
    "MAX_REQUESTS = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_PHASE5_A = \"\"\"\n",
    "You are an expert in Android security and privacy.\n",
    "\n",
    "### Task Description  \n",
    "Some Android API methods may return data that can be exploited by an attacker, either directly or through inference from correlated information.\n",
    "\n",
    "You will receive a list of privacy-related labels that describe how such return values can be used to infer private user information.\n",
    "\n",
    "A user has described their final task in natural language as follows:\n",
    "\"{}\"\n",
    "\n",
    "Your goal is to identify and return only the relevant labels from the list. These may include:\n",
    "- DIRECT MATCHES to the described task.\n",
    "- INDIRECT MATCHES where data can be used to infer the target information related to the task.\n",
    "\n",
    "Think step by step and use your understanding of inference, data correlation, and side-channel analysis.\n",
    "\n",
    "### Output Constraints:\n",
    "- Return only the relevant labels as a list in square brackets, e.g., [\"LABEL_1\", \"LABEL_2\"].\n",
    "- Do not include explanations, reasoning, or formatting outside the list.\n",
    "\n",
    "### Privacy Labels:\n",
    "{}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_PHASE5_B = \"\"\"\n",
    "You are an Android security expert specializing in taint analysis with FlowDroid.  \n",
    "\n",
    "### **Task Description**  \n",
    "The return value of some Android API methods can be exploited by an attacker for an advantage, either in isolation or in combination with other data.\n",
    "\n",
    "A FlowDroid user has described their final task in natural language as follows:  \n",
    "\"{}\"\n",
    "\n",
    "Your task is to determine whether the given method should be classified as a **SOURCE** in the context of the user's task i.e., whether running FlowDroid on this method may result in a taint flow from the method that satisfies the user's task. \n",
    "\n",
    "### **Instructions**  \n",
    "1. Base your classification solely on the user's task description.\n",
    "2. Analyze the method strictly within the provided context.  \n",
    "3. Classify the method as either:\n",
    "\t- **SOURCE** (if it serves as an information source in the given context).  \n",
    "\t- **NOT_SOURCE** (if it does not).  \n",
    "4. **Output only the classification label** (**SOURCE** or **NOT_SOURCE**)‚Äîno explanations. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### USER TASK 1 ###\n",
    "USER_TASK_1_A = \"Which Android API methods return information which could reveal the user‚Äôs physical location? This includes precise location (e.g., GPS, Wi-Fi, cell tower data), approximate location (e.g., IP address, network configuration), or broad geographic indicators such as timezone, country, SIM/network region, settings. Include any methods that could assist an attacker in estimating user location at any granularity, even indirectly.\"\n",
    "USER_TASK_1_B = \"Which APIs return data that an attacker could use to infer the location of the user/device, even indirectly?\"\n",
    "USER_TASK_1_C = \"Which APIs return location data?\"\n",
    "\n",
    "### USER TASK 2 ###\n",
    "USER_TASK_2 = \"Which APIs access microphone-related data (e.g., audio input state or stream), which could be useful for an attacker aiming to eavesdrop or gather information about the user‚Äôs environment without their consent?\"\n",
    "\n",
    "### USER TASK 3 ###\n",
    "USER_TASK_3 = \"Which APIs interact with clipboard content, potentially exposing copied sensitive information such as passwords, addresses, or personal data?\"\n",
    "\n",
    "# Choose the User Task\n",
    "USER_TASK = USER_TASK_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the clusterID\n",
    "clusterID = \"clusterID_{}\".format(N_CLUSTERS)\n",
    "\n",
    "# Print info\n",
    "print(\"--- üîπ CLUSTERD ID: {}\".format(clusterID))\n",
    "\n",
    "# Select the privacy labels for a specific clusterID\n",
    "privacyLabelsIDs = list(privacyLabelsDict.keys())\n",
    "print(\"--- ‚≠ï All Privacy Labels [{}]: {}\".format(len(privacyLabelsIDs), privacyLabelsIDs))\n",
    "\n",
    "# Insert the list of privacy labels in the prompt\n",
    "promptPhase5a = PROMPT_PHASE5_A.format(USER_TASK, \"\\n\".join([\"- {}: {}\".format(key, privacyLabelsDict[key]) for key in privacyLabelsDict]))\n",
    "\n",
    "# Testing purposes\n",
    "print(\"--- ‚≠ï New Prompt Phase 5A:\")\n",
    "print(promptPhase5a)\n",
    "\n",
    "# To store the filtered privacy labels\n",
    "filteredPrivacyLabels = []\n",
    "\n",
    "for iteration in range(MAX_REQUESTS):\n",
    "\ttry:\n",
    "\t\t# Send the request\tto the LLM\n",
    "\t\tanswer = llmInterface.sendRequest(promptPhase5a)\n",
    "\n",
    "\t\t# Testing purposes\n",
    "\t\tprint(\"--- ‚≠ï LLM Response\")\n",
    "\t\tprint(answer)\n",
    "\t\t\t\t\n",
    "\t\t# Check if the answer is a valid list\n",
    "\t\tfilteredPrivacyLabels = eval(answer)\n",
    "\n",
    "\t\t# Check if the answer is a valid list\n",
    "\t\tif isinstance(filteredPrivacyLabels, list):\n",
    "\t\t\tbreak\n",
    "\n",
    "\texcept (SyntaxError, NameError):\n",
    "\t\tcontinue\n",
    "\n",
    "if iteration == MAX_REQUESTS - 1:\n",
    "\tprint(\"--- ‚ùå Reached max iterations without valid response\")\n",
    "\n",
    "# Print the filtered privacy labels\n",
    "print(\"--- ‚≠ï Filtered Privacy Labels [{}]: {}\".format(len(filteredPrivacyLabels), filteredPrivacyLabels))\n",
    "\n",
    "# Filter the android methods\n",
    "filteredMethodsDF = methodsDF[methodsDF[clusterID].isin(filteredPrivacyLabels)]\n",
    "print(\"--- #Ô∏è‚É£ Filtered Methods Size: {}\".format(filteredMethodsDF.shape[0]))\n",
    "\n",
    "# # TEST\n",
    "# #filteredMethodsDF = filteredMethodsDF.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üñ•Ô∏è 3B) Using LLM to refine final list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "promptPhase5b = PROMPT_PHASE5_B.format(USER_TASK)\n",
    "print(\"\\n--- ‚≠ï New Prompt Phase 5B:\")\n",
    "print(promptPhase5b)\n",
    "\n",
    "# Process each row in the DataFrame\n",
    "def processRow(row):\n",
    "\t# Get information from the row\n",
    "\tsha256          = row['sha256']\n",
    "\tmethodSignature = row['methodSignature']\n",
    "\n",
    "\tprint(\"\\n--- üîç Android Method: {}\".format(methodSignature))\n",
    "\n",
    "\t# Retrieve documentation and sourceCode\n",
    "\tdocumentationPath \t= os.path.join(DOCUMENTATION_PATH, \"{}.txt\".format(sha256))\n",
    "\twith open(documentationPath, 'r') as DocFile:\n",
    "\t\tdocumentation = DocFile.read()\n",
    "\t\t\n",
    "\n",
    "\t# Create an object representing the Android Method\n",
    "\tandroidMethod = AnalysisUtils.AndroidMethod(sha256, methodSignature, documentation)\n",
    "\n",
    "\t# Create the prompt\n",
    "\tprompt = androidMethod.addAllToPrompt(promptPhase5b)\n",
    "\n",
    "\t# Testing purposes\n",
    "\tprint(\"--- üîç Prompt: {}\".format(prompt))\n",
    "\n",
    "\tlabelFrequency = AnalysisUtils.labelAndroidMethod(llmInterface, prompt, ['SOURCE','NOT_SOURCE'], NUM_REPLIES, MAX_REQUESTS)\n",
    "\tllmLabel \t   = AnalysisUtils.getMostFrequentLabel(labelFrequency)\n",
    "\tprint(\"--- üîç Label Frequency     :\", labelFrequency)\n",
    "\tprint(\"--- üè∑Ô∏è Most Frequent Label :\", llmLabel)\n",
    "\n",
    "\t# Save the label\n",
    "\tfilteredMethodsDF.at[row.name, 'llmLabel_FINAL'] = llmLabel\n",
    "\n",
    "\tprint(\"\\n\" + \"---\" * 20)\n",
    "\n",
    "# Apply the function to the DataFrame\n",
    "_ = filteredMethodsDF.apply(processRow, axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üíæ 4] Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to save the results\n",
    "RESULTS_PATH = \"./0_PipelineData/\"\n",
    "\n",
    "# Create folder if it does not exist\n",
    "if not os.path.exists(RESULTS_PATH):\n",
    "\tos.makedirs(RESULTS_PATH)\n",
    "\tprint(\"--- üìÅüÜï Folder created       : {}\\n\".format(RESULTS_PATH))\n",
    "\n",
    "# Save results to a CSV File\n",
    "filteredMethodsDF.drop(columns=clusterID, inplace=True)\n",
    "filteredMethodsDF.to_csv(os.path.join(RESULTS_PATH, \"sourceMethods.csv\"), index=False)\n",
    "\n",
    "# Filter rows where llmLabel_FINAL is \"SOURCE\" and save to a text file\n",
    "sourceMethods = filteredMethodsDF[filteredMethodsDF['llmLabel_FINAL'] == 'SOURCE']['methodSignature']\n",
    "with open(os.path.join(RESULTS_PATH, \"sourceMethods.txt\"), 'w') as file:\n",
    "\tfile.write(\"\\n\".join(sourceMethods))\n",
    "\n",
    "print(\"--- üíæ Results saved successfully at: {}\".format(RESULTS_PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### üîö End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endTime = datetime.datetime.now()\n",
    "print(\"\\nüîö --- END:  {} --- üîö\".format(endTime.strftime(\"%Y-%m-%d %H:%M:%S\")))\n",
    "\n",
    "# Assuming endTime and initTime are datetime objects\n",
    "totalTime = endTime - initTime\n",
    "hours     = totalTime.total_seconds() // 3600\n",
    "minutes   = (totalTime.total_seconds() % 3600) // 60\n",
    "seconds   = totalTime.total_seconds() % 60\n",
    "print(\"‚è±Ô∏è --- Time: {:02d} hours and {:02d} minutes [{:02d} seconds] --- ‚è±Ô∏è\".format(int(hours), int(minutes), int(totalTime.total_seconds())))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "marcoEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
